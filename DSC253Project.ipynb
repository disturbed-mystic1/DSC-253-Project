{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 253 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Financial Agent RAG "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTALLATING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "!pip install sec_api\n",
    "!pip install -U langchain\n",
    "!pip install -U langchain-community\n",
    "!pip install -U sentence-transformers\n",
    "!pip install -U faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS & CREDENTIAL SETUP\n",
    "\n",
    "HuggingFace Token Found: https://huggingface.co/settings/tokens\n",
    "Free SEC API Key Here: https://sec-api.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"\"\n",
    "sec_api_key = \"\"\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from sec_api import ExtractorApi, QueryApi\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL & TOKENIZER INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    "    token=hf_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPLY LoRA ADAPTERS FOR FINE-TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPARATION & PROMPT TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "Below is a user question, paired with retrieved context. Write a response that answers the question, with specific details. <|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "### Question:\n",
    "{}\n",
    "### Context:\n",
    "{}\n",
    "\n",
    "<|eot_id|>\n",
    "\n",
    "### Response: <|start_header_id|>assistant<|end_header_id|>\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    q = examples[\"question\"]\n",
    "    c = examples[\"context\"]\n",
    "    a = examples[\"answer\"]\n",
    "    texts = []\n",
    "    for question, context, response in zip(q, c, a):\n",
    "        text = ft_prompt.format(question, context, response) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = load_dataset(\"virattt/llama-3-8b-financialQA\", split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=2048,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Uncomment to train:\n",
    "# trainer_stats = trainer.train()\n",
    "# You may require a weights and biases api key\n",
    "# Uncomment to save the trained model and change it to your directory:\n",
    "# model.save_pretrained(\"/content/drive/MyDrive\")\n",
    "# tokenizer.save_pretrained(\"/content/drive/MyDrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFERENCE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(question, context):\n",
    "    inputs = tokenizer([ft_prompt.format(question, context, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=64,\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs)\n",
    "\n",
    "def extract_response(text):\n",
    "    text = text[0]\n",
    "    start_token = \"### Response: <|start_header_id|>assistant<|end_header_id|>\"\n",
    "    end_token = \"<|eot_id|>\"\n",
    "    start_index = text.find(start_token) + len(start_token)\n",
    "    end_index = text.find(end_token, start_index)\n",
    "    if start_index == -1 or end_index == -1:\n",
    "        return None\n",
    "    return text[start_index:end_index].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEC DATA EXTRACTION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filings(ticker):\n",
    "    queryApi = QueryApi(api_key=sec_api_key)\n",
    "    query = {\n",
    "      \"query\": f\"ticker:{ticker} AND formType:\\\"10-K\\\"\",\n",
    "      \"from\": \"0\",\n",
    "      \"size\": \"1\",\n",
    "      \"sort\": [{ \"filedAt\": { \"order\": \"desc\" } }]\n",
    "    }\n",
    "    filings = queryApi.get_filings(query)\n",
    "    filing_url = filings[\"filings\"][0][\"linkToFilingDetails\"]\n",
    "    extractorApi = ExtractorApi(api_key=sec_api_key)\n",
    "    onea_text = extractorApi.get_section(filing_url, \"1A\", \"text\")\n",
    "    seven_text = extractorApi.get_section(filing_url, \"7\", \"text\")\n",
    "    return onea_text + \"\\n\\n\" + seven_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMBEDDINGS & VECTOR DATABASE SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "    model_kwargs={'device':'cuda'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "ticker = input(\"Enter ticker symbol (e.g. AAPL): \")\n",
    "filing_data = get_filings(ticker)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=500,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "split_data = text_splitter.create_documents([filing_data])\n",
    "db = FAISS.from_documents(split_data, embeddings)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RETRIEVAL FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query):\n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "    return [doc.page_content for doc in retrieved_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INTERACTIVE QUERY LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_question = input(f\"Ask about {ticker}'s 10-K (type 'x' to exit): \")\n",
    "    if user_question.lower() == \"x\":\n",
    "        break\n",
    "    context = retrieve_context(user_question)\n",
    "    resp = inference(user_question, context)\n",
    "    parsed_resp = extract_response(resp)\n",
    "    print(f\"Answer: {parsed_resp}\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "\n",
    "1. https://colab.research.google.com/drive/1eisDW1zTuHgHzoPS8o8AKfPThQ3rWQ2P#scrollTo=cK8V6P9uYFJz \n",
    "2. https://docs.unsloth.ai/get-started/unsloth-notebooks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSC253",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
